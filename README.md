# What is LLM Benchmark
# Benchmark Category
There are 8 key LLM Benchmarks across the 4 most critical domains (Language Understanding, Reasoning, Coding, and Conversation). These benchmarks are widely utilized in industry applications and are frequently cited in technical reports. They include:

* **TruthfulQA** — Truthfulness
* **MMLU** — Language understanding
* **HellaSwag** — Commonsense reasoning
* **BIG-Bench Hard** — Challenging reasoning tasks
* **HumanEval** — Coding challenges
* **CodeXGLUE** — Programming tasks
* **Chatbot Arena** — Human-ranked ELO-based benchmark
* **MT Bench** — Complex conversational ability

![image](https://github.com/user-attachments/assets/4e4abbe2-e673-4f33-82f8-f0d76fc63e5f)
Source: [confident-ai.com](https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond)

## LLM Benchmark Leaderboard

| Task | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku | GPT-4 | GPT-3.5 | Gemini 1.0 Ultra | Gemini 1.0 Pro |
|---|---|---|---|---|---|---|---|
| **Undergraduate level knowledge** <br> MMLU | 86.8% | 79.0% | 75.2% | 86.4% | 70.0% | 83.7% | 71.8% |
| **Graduate level reasoning** <br> GPQA, Diamond | 50.4% | 40.4% | 33.3% | 35.7% | 28.1% | - | - |
| **Grade school math** <br> GSM8K | 95.0% | 92.3% | 88.9% | 92.0% | 57.1% | 94.4% | 86.5% |
| **Math problem-solving** <br> MATH | 60.1% | 43.1% | 38.9% | 52.9% | 34.1% | 53.2% | 32.6% |
| **Multilingual math** <br> MGSM | 90.7% | 83.5% | 75.1% | 74.5% | - | 79.0% | 63.5% |
| **Code** <br> HumanEval | 84.9% | 73.0% | 75.9% | 67.0% | 48.1% | 74.4% | 67.7% |
| **Reasoning over text** <br> DROP, F1 score | 83.1 | 78.9 | 78.4 | 80.9 | 64.1 | 82.4 | 74.1 |
| **Mixed evaluations** <br> BIG-Bench-Hard | 86.8% | 82.9% | 73.7% | 83.1% | 66.6% | 83.6% | 75.0% | 


# Reference
* https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond





