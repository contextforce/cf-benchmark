# What is LLM Benchmark
# Benchmark Category
![image](https://github.com/user-attachments/assets/4e4abbe2-e673-4f33-82f8-f0d76fc63e5f)
Source: [confident-ai.com](https://www.confident-ai.com/blog/llm-benchmarks-mmlu-hellaswag-and-beyond)

## LLM Benchmark Leaderboard

| Task | Claude 3 Opus | Claude 3 Sonnet | Claude 3 Haiku | GPT-4 | GPT-3.5 | Gemini 1.0 Ultra | Gemini 1.0 Pro |
|---|---|---|---|---|---|---|---|
| **Undergraduate level knowledge** <br> MMLU | 86.8% <br> 5-shot | 79.0% <br> 5-shot | 75.2% <br> 5-shot | 86.4% <br> 5-shot | 70.0% <br> 5-shot | 83.7% <br> 5-shot | 71.8% <br> 5-shot |
| **Graduate level reasoning** <br> GPQA, Diamond | 50.4% <br> 0-shot CoT | 40.4% <br> 0-shot CoT | 33.3% <br> 0-shot CoT | 35.7% <br> 0-shot CoT | 28.1% <br> 0-shot CoT | - | - |
| **Grade school math** <br> GSM8K | 95.0% <br> 0-shot CoT | 92.3% <br> 0-shot CoT | 88.9% <br> 0-shot CoT | 92.0% <br> 5-shot CoT | 57.1% <br> 5-shot | 94.4% <br> Majl@32 | 86.5% <br> Majl@32 |
| **Math problem-solving** <br> MATH | 60.1% <br> 0-shot CoT | 43.1% <br> 0-shot CoT | 38.9% <br> 0-shot CoT | 52.9% <br> 4-shot | 34.1% <br> 4-shot | 53.2% <br> 4-shot | 32.6% <br> 4-shot |
| **Multilingual math** <br> MGSM | 90.7% <br> 0-shot | 83.5% <br> 0-shot | 75.1% <br> 0-shot | 74.5% <br> 5-shot | - | 79.0% <br> 0-shot | 63.5% <br> 0-shot |
| **Code** <br> HumanEval | 84.9% <br> 0-shot | 73.0% <br> 0-shot | 75.9% <br> 0-shot | 67.0% <br> 0-shot | 48.1% <br> 0-shot | 74.4% <br> 0-shot | 67.7% <br> 0-shot |
| **Reasoning over text** <br> DROP, F1 score | 83.1 <br> 3-shot | 78.9 <br> 3-shot | 78.4 <br> 3-shot | 80.9 <br> 3-shot | 64.1 <br> 3-shot | 82.4 <br> Variable shots | 74.1 <br> Variable shots |
| **Mixed evaluations** <br> BIG-Bench-Hard | 86.8% <br> 3-shot CoT | 82.9% <br> 3-shot CoT | 73.7% <br> 3-shot CoT | 83.1% <br> 3-shot CoT | 66.6% <br> 3-shot CoT | 83.6% <br> 3-shot CoT | 75.0% <br> 3-shot CoT | 



